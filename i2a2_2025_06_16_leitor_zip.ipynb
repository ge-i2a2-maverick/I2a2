{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GfkLEzQjo6en",
        "outputId": "71d993c1-57b1-4435-a74c-bbef7c7ee81f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.1/186.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.1/374.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.4/207.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.4/115.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-experimental llama-index-llms-groq gradio -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1Ro4zgocncu9"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "import zipfile\n",
        "import os\n",
        "import io\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "\n",
        "from datetime import datetime\n",
        "from google.colab import userdata\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.experimental.query_engine import PandasQueryEngine\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.experimental.query_engine.pandas import PandasInstructionParser\n",
        "from llama_index.core.query_pipeline import (QueryPipeline as QP, Link, InputComponent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XHCnWqKlducc",
        "outputId": "3e6d9f64-5e05-4ee6-dcce-bcf65428c769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://bcca1afba6f15a636c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bcca1afba6f15a636c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
            "query_str: Qual a maior notas fiscais e que itens foram comprados?\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
            "query_str: Qual a maior notas fiscais e que itens foram comprados?\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm1 with input: \n",
            "messages: Você está trabalhando com um dataframe do pandas em Python chamado `df`.\n",
            "Aqui estão os detalhes das colunas do dataframe:\n",
            "`CHAVE DE ACESSO`: object\n",
            "`MODELO_x`: object\n",
            "`SÉRIE_x`: int64\n",
            "`NÚMERO_x`: int6...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
            "input: assistant: df.loc[df['VALOR TOTAL'].idxmax()]\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
            "query_str: Qual a maior notas fiscais e que itens foram comprados?\n",
            "pandas_instructions: assistant: df.loc[df['VALOR TOTAL'].idxmax()]\n",
            "pandas_output: CHAVE DE ACESSO                       35240158309709000153550040001357171266796999\n",
            "MODELO_x                         55 - NF-E EMITIDA EM SUBSTITUIÇÃO AO MODELO 1 ...\n",
            "SÉRIE_x                           ...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm2 with input: \n",
            "messages: Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\n",
            "Responda de forma natural, sem introduções como 'A resposta é:' ou algo semelhante...\n",
            "\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "api_key = userdata.get('GROQ_API')\n",
        "\n",
        "# Configuração do modelo\n",
        "llm = Groq(model=\"llama3-70b-8192\", api_key=api_key)\n",
        "\n",
        "# Pipeline de consulta\n",
        "'''Função para obter uma descrição das colunas do DataFrame'''\n",
        "def descricao_colunas(df):\n",
        "    descricao = '\\n'.join([f\"`{col}`: {str(df[col].dtype)}\" for col in df.columns])\n",
        "    return \"Aqui estão os detalhes das colunas do dataframe:\\n\" + descricao\n",
        "\n",
        "'''Definição de módulos da pipeline'''\n",
        "def pipeline_consulta(df):\n",
        "    instruction_str = (\n",
        "        \"1. Converta a consulta para código Python executável usando Pandas.\\n\"\n",
        "        \"2. A linha final do código deve ser uma expressão Python que possa ser chamada com a função `eval()`.\\n\"\n",
        "        \"3. O código deve representar uma solução para a consulta.\\n\"\n",
        "        \"4. IMPRIMA APENAS A EXPRESSÃO.\\n\"\n",
        "        \"5. Não coloque a expressão entre aspas.\\n\")\n",
        "\n",
        "    pandas_prompt_str = (\n",
        "        \"Você está trabalhando com um dataframe do pandas em Python chamado `df`.\\n\"\n",
        "        \"{colunas_detalhes}\\n\\n\"\n",
        "        \"Este é o resultado de `print(df.head())`:\\n\"\n",
        "        \"{df_str}\\n\\n\"\n",
        "        \"Siga estas instruções:\\n\"\n",
        "        \"{instruction_str}\\n\"\n",
        "        \"Consulta: {query_str}\\n\\n\"\n",
        "        \"Expressão:\"\n",
        ")\n",
        "\n",
        "    response_synthesis_prompt_str = (\n",
        "       \"Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\\n\"\n",
        "       \"Responda de forma natural, sem introduções como 'A resposta é:' ou algo semelhante.\\n\"\n",
        "       \"Consulta: {query_str}\\n\\n\"\n",
        "       \"Instruções do Pandas (opcional):\\n{pandas_instructions}\\n\\n\"\n",
        "       \"Saída do Pandas: {pandas_output}\\n\\n\"\n",
        "       \"Resposta: \\n\\n\"\n",
        "       \"Ao final, exibir o código usado em para gerar a resposta, no formato: O código utilizado foi `{pandas_instructions}`\"\n",
        "    )\n",
        "\n",
        "    pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
        "    instruction_str=instruction_str,\n",
        "    df_str=df.head(5),\n",
        "    colunas_detalhes=descricao_colunas(df)\n",
        ")\n",
        "\n",
        "    pandas_output_parser = PandasInstructionParser(df)\n",
        "    response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
        "\n",
        "    '''Criação do Query Pipeline'''\n",
        "    qp = QP(\n",
        "        modules={\n",
        "            \"input\": InputComponent(),\n",
        "            \"pandas_prompt\": pandas_prompt,\n",
        "            \"llm1\": llm,\n",
        "            \"pandas_output_parser\": pandas_output_parser,\n",
        "            \"response_synthesis_prompt\": response_synthesis_prompt,\n",
        "            \"llm2\": llm,\n",
        "        },\n",
        "        verbose=True,\n",
        "    )\n",
        "    qp.add_chain([\"input\", \"pandas_prompt\", \"llm1\", \"pandas_output_parser\"])\n",
        "    qp.add_links(\n",
        "        [\n",
        "            Link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\"),\n",
        "            Link(\"llm1\", \"response_synthesis_prompt\", dest_key=\"pandas_instructions\"),\n",
        "            Link(\"pandas_output_parser\", \"response_synthesis_prompt\", dest_key=\"pandas_output\"),\n",
        "        ]\n",
        "    )\n",
        "    qp.add_link(\"response_synthesis_prompt\", \"llm2\")\n",
        "    return qp\n",
        "\n",
        "def extrair_zip_e_carregar_csvs_em_memoria(caminho_zip):\n",
        "    \"\"\"\n",
        "    Extrai todos os arquivos CSV de um ZIP e os carrega em DataFrames Pandas em memória.\n",
        "    Retorna um dicionário onde a chave é o nome do arquivo CSV (sem caminho) e o valor é o DataFrame.\n",
        "    \"\"\"\n",
        "    if not zipfile.is_zipfile(caminho_zip):\n",
        "        raise ValueError(\"O arquivo fornecido não é um arquivo ZIP válido.\")\n",
        "\n",
        "    dfs = {}\n",
        "    with zipfile.ZipFile(caminho_zip, 'r') as zip_ref:\n",
        "        csv_files_in_zip = [name for name in zip_ref.namelist() if name.lower().endswith('.csv')]\n",
        "\n",
        "        if not csv_files_in_zip:\n",
        "            raise FileNotFoundError(\"Nenhum arquivo CSV encontrado dentro do arquivo ZIP.\")\n",
        "\n",
        "        for csv_file_name in csv_files_in_zip:\n",
        "            try:\n",
        "                with zip_ref.open(csv_file_name) as file:\n",
        "                    # Lê o arquivo CSV diretamente da memória\n",
        "                    df = pd.read_csv(io.TextIOWrapper(file, 'utf-8'))\n",
        "                    # Usa apenas o nome do arquivo, sem o caminho completo dentro do zip\n",
        "                    df_name = os.path.basename(csv_file_name)\n",
        "                    dfs[df_name] = df\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao ler o CSV '{csv_file_name}' do ZIP: {e}\")\n",
        "                # Opcional: ignorar arquivos CSV que não podem ser lidos ou levantar um erro mais específico\n",
        "\n",
        "    if not dfs:\n",
        "        raise ValueError(\"Nenhum arquivo CSV válido pôde ser carregado do ZIP.\")\n",
        "\n",
        "    return dfs\n",
        "\n",
        "# Função para carregar os dados\n",
        "def carregar_dados(caminho_arquivo, df_estado):\n",
        "    if caminho_arquivo is None or caminho_arquivo == \"\":\n",
        "        return \"Por favor, faça o upload de um arquivo (CSV ou ZIP) para analisar.\", pd.DataFrame(), df_estado\n",
        "\n",
        "    try:\n",
        "        if caminho_arquivo.lower().endswith('.zip'):\n",
        "            dfs_dict = extrair_zip_e_carregar_csvs_em_memoria(caminho_arquivo)\n",
        "            # Para exibição, mostramos a cabeça do primeiro DataFrame encontrado\n",
        "            first_df_name = list(dfs_dict.keys())[0]\n",
        "\n",
        "            df1_name = list(dfs_dict.keys())[0]\n",
        "            df2_name = list(dfs_dict.keys())[1]\n",
        "            df1 = dfs_dict[df1_name]\n",
        "            df2 = dfs_dict[df2_name]\n",
        "            # Aqui você faria uma junção, agregação ou comparação entre df1 e df2\n",
        "            try:\n",
        "                merged_df = pd.merge(df1, df2, on='CHAVE DE ACESSO', how='inner')\n",
        "            except KeyError:\n",
        "                response_content += \"\\nNão foi possível fazer o merge direto por 'coluna_comum'.\"\n",
        "            return \"Arquivo carregado com sucesso!\", merged_df.head(), merged_df\n",
        "            #(f\"Arquivo ZIP descompactado e {len(dfs_dict)} CSV(s) carregados com sucesso! \"\n",
        "            #        f\"Exibindo a cabeça do '{first_df_name}'.\", dfs_dict[first_df_name].head(), dfs_dict)\n",
        "        elif caminho_arquivo.lower().endswith('.csv'):\n",
        "            df = pd.read_csv(caminho_arquivo)\n",
        "            # Se for um único CSV, armazena em um dicionário para consistência\n",
        "            return \"Arquivo carregado com sucesso!\", df.head(), df\n",
        "        else:\n",
        "            return \"Formato de arquivo não suportado. Por favor, faça upload de um arquivo CSV ou ZIP.\", pd.DataFrame(), df_estado\n",
        "    except Exception as e:\n",
        "        return f\"Erro ao carregar arquivo: {str(e)}\", pd.DataFrame(), df_estado\n",
        "\n",
        "# Função para processar a pergunta\n",
        "def processar_pergunta(pergunta, df_estado):\n",
        "    if df_estado is not None and pergunta:\n",
        "        qp = pipeline_consulta(df_estado)\n",
        "        resposta = qp.run(query_str=pergunta)\n",
        "        return resposta.message.content\n",
        "    return \"\"\n",
        "\n",
        "# Função para resetar a aplicação\n",
        "def resetar_aplicação():\n",
        "    return None, \"A aplicação foi resetada. Por favor, faça upload de um novo arquivo CSV.\", pd.DataFrame(), \"\", None, [], \"\"\n",
        "\n",
        "# Criação da interface gradio\n",
        "with gr.Blocks(theme=\"Soft\") as app:\n",
        "\n",
        "    # Título da app\n",
        "    gr.Markdown(\"# Analisando os dados🔎🎲\")\n",
        "\n",
        "    # Descrição\n",
        "    gr.Markdown(\"\"\"\n",
        "    Carregue um arquivo ZIP e faça perguntas sobre os dados. A cada pergunta, você poderá\n",
        "    visualizar a resposta. Se você quiser analisar um novo dataset, basta clicar em \"Quero analisar outro dataset\" ao final da página.\n",
        "    \"\"\")\n",
        "\n",
        "    # Campo de entrada de arquivos\n",
        "    input_arquivo = gr.File(file_count=\"single\", type=\"filepath\", label=\"Upload CSV\", file_types=[\".csv\", \".zip\"])\n",
        "\n",
        "    # Status de upload\n",
        "    upload_status = gr.Textbox(label=\"Status do Upload:\")\n",
        "\n",
        "    # Tabela de dados\n",
        "    tabela_dados = gr.DataFrame()\n",
        "\n",
        "    # Exemplos de perguntas\n",
        "    gr.Markdown(\"\"\"\n",
        "    Exemplos de perguntas:\n",
        "    1. Qual a origem que tem mais itens?\n",
        "    2. Qual o valor total das notas fiscais ?\n",
        "    3. Qual é a origem que mais faturou em notas fiscais?\n",
        "    4. Qual a maior notas fiscais e que itens foram comprados?\n",
        "    \"\"\")\n",
        "\n",
        "    # Campo de entrada de texto\n",
        "    input_pergunta = gr.Textbox(label=\"Digite sua pergunta sobre os dados\")\n",
        "\n",
        "    # Botão de envio posicionado após a pergunta\n",
        "    botao_submeter = gr.Button(\"Enviar\")\n",
        "\n",
        "    # Componente de resposta\n",
        "    output_resposta = gr.Textbox(label=\"Resposta\")\n",
        "\n",
        "    # Botão para resetar a aplicação\n",
        "    botao_resetar = gr.Button(\"Quero analisar outro dataset!\")\n",
        "\n",
        "    # Gerenciamento de estados\n",
        "    df_estado = gr.State(value=None)\n",
        "\n",
        "    # Conectando funções aos componentes\n",
        "    input_arquivo.change(fn=carregar_dados,\n",
        "                         inputs=[input_arquivo, df_estado],\n",
        "                         outputs=[upload_status, tabela_dados, df_estado])\n",
        "\n",
        "    botao_submeter.click(fn=processar_pergunta,\n",
        "                         inputs=[input_pergunta, df_estado],\n",
        "                         outputs=output_resposta)\n",
        "\n",
        "    botao_resetar.click(fn=resetar_aplicação,\n",
        "                        inputs=[],\n",
        "                        outputs=[input_arquivo, upload_status, tabela_dados, output_resposta, input_pergunta])\n",
        "\n",
        "    app.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}